{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"CSCD84","text":"<p>Notes from the CSCD84 course taken Winter 2023.</p>"},{"location":"1-UCS-and-Heuristic-Search/","title":"1 UCS and Heuristic Search","text":"<ul> <li>Review of lecture 1:<ul> <li>UCS (Uniform Cost Search)</li> </ul> </li> <li>Heuristic Search (A* Search)<ul> <li>Conditions of Heuristic Search</li> <li>Cost of traversal<ul> <li>For the route navigation problem</li> </ul> </li> </ul> </li> <li>Negative Edge Weights         - Possible solution         - Bellman-Ford Algorithm</li> </ul>"},{"location":"1-UCS-and-Heuristic-Search/#review-of-lecture-1","title":"Review of lecture 1:","text":"<p>Discussed states, represented as nodes, which correspond to configurations. The AI can travel between states along weighted edges.     Right now we just consider the weighted edges and try to minimize the path to the goal state.</p> <p>Recall BFS and DFS from B63     Both explore all nodes \\(\\mathcal{O}(N)\\) , and all edges \\(\\mathcal{O}(E)\\) for a total of \\(\\mathcal{O}(|N|+|E|)\\) time</p>"},{"location":"1-UCS-and-Heuristic-Search/#ucs-uniform-cost-search","title":"UCS (Uniform Cost Search)","text":"<ul> <li>Expand in order of increasing cost from the start state (S), using cost function \\(g(s)\\) which is simply the distance from the start node to current node </li> <li>Uses a priority queue</li> <li>It's Dijkstra's but stops when we reach a goal state, instead of calculating shortest path to every node in the graph</li> <li> <p>All edges &gt; 0. Will work with edge weight of 0, but realistically no action costs 0</p> </li> <li> <p>Will be in this weeks assignment</p> </li> </ul>"},{"location":"1-UCS-and-Heuristic-Search/#heuristic-search-a-search","title":"Heuristic Search (A* Search)","text":"<ul> <li>Heuristic simply means a guess</li> <li>Used to reduce the amount of search required to reach goal state</li> </ul> <p>Heuristic Search, also called \\(A^*\\) (A-star) search is a powerful and extensively used search algorithm that makes use of a heuristic function to guess which nodes in the search tree are likely to be closer to a goal state, so those nodes can be expanded first.</p> <p>Because it is a guess, it can not be used exclusively to guide search expansion order (if you think about it, in order to know exactly how far each node is from the goal, we'd have to have completed the search process on the entire tree!). Instead, \\(A^*\\) search uses the heuristic function in combination with the actual cost of getting to a node from the initial state (this value is updated as nodes are expanded, just like with UCS). The heuristic cost used by A* search to determine expansion order is:</p> \\[ f(n)=g(n)+h(n) \\] <p>Where \\(f(n)\\) is the heuristic cost for some node \\(\\mathbf{n}\\), and it consists of \\(\\mathbf{g}(\\mathbf{n})\\) - the actual cost of reaching node \\(\\mathbf{n}\\) from the initial node, and \\(\\mathbf{h}(\\mathbf{n})\\) a guess of the cost to get to [the/a] goal state from \\(n\\). The \\(A^*\\) search algorithm is identical to UCS, but we use the heuristic cost \\(h(n)\\) to order nodes in the priority queue. Otherwise there's no change! So if you have implemented UCS, implementing A* is straightforward, you just need to add a suitable heuristic function.</p> <p>In the example above, a good guess would be to use the Manhattan distance between a node and the goal. This happens to be a lucky guess for this problem because the Manhattan distance is the actual distance between two nodes - it is very often not the case that we can find such an appropriate guess, so we'll talk about what makes a good guess below. For now, let's see how things change once we introduce the heuristic cost for each node into the search process.</p> <p>We try to create a heuristic cost that guesses the cost of the path from node N to G (goal state). In the city grid example, it can simply be the Manhattan Distance.</p> <p> This isn't trivial because streets may have conditions, such as speed limits, one way streets, etc. </p>"},{"location":"1-UCS-and-Heuristic-Search/#conditions-of-heuristic-search","title":"Conditions of Heuristic Search","text":"<ul> <li> <p>\\(h(n)\\) is admissible if</p> <ul> <li>\\(h(n) \\le h^*(n) \\quad \\forall n\\)<ul> <li>Where \\(h^*(n)\\) is the true cost from \\(n\\) to goal</li> </ul> </li> <li>\\(h(n) \\ne 0 \\quad \\forall n\\) </li> </ul> </li> <li> <p>\\(h^*(n)\\) is always admissible</p> </li> <li> <p>When constructiong a heuristic, need to prove admissibility and show that it is not worse than no heuristic (UCS). UCS guarantees optimality in terms of path, so huerisitc search may not always be optimal but being able to reduce complexity/algo time is important.</p> </li> </ul>"},{"location":"1-UCS-and-Heuristic-Search/#cost-of-traversal","title":"Cost of traversal","text":"<p>\\(f(n) = g(n) + f(n)\\) - \\(g(n)\\) is the weight of the path, but in a navigation problem, the weights may change due to traffic. Therefore, it is very difficult to determine whether given path is optimal.     - We can only solve based on the current information     - Ex could update weights based on the past k drivers (Waze will estimate your travel time based on the speed of other drivers, so you may need to speed if you want to make that time)</p>"},{"location":"1-UCS-and-Heuristic-Search/#for-the-route-navigation-problem","title":"For the route navigation problem","text":"<ul> <li>Can create a heuristic based on:<ul> <li>increase if: <ul> <li>euclidean distance is greater</li> <li>there are traffic lights</li> </ul> </li> <li>decreasing if: <ul> <li>path has higher speed</li> </ul> </li> <li>many other factors to consider for the heuristic</li> </ul> </li> </ul>"},{"location":"1-UCS-and-Heuristic-Search/#negative-edge-weights","title":"Negative Edge Weights","text":"<p>Some graphs could have negative edge weights.  - UCS/A* will no longer guarantee the optimal cost and path     - But we may not care if we don't have the most optimal path - Need to ensure there are no loops, or that the user cannot loop</p>"},{"location":"1-UCS-and-Heuristic-Search/#possible-solution","title":"Possible solution","text":"<p>Could add a \\(k\\), where \\(k\\) is the most negative edge     This guarantees all edges are non-negative (or positive?)     This will however change the optimal solution We actually need to change the search algorithm</p>"},{"location":"1-UCS-and-Heuristic-Search/#bellman-ford-algorithm","title":"Bellman-Ford Algorithm","text":"<p>Start with \\(V[n] = \\infty\\) for all nodes \\(n\\) where \\(V[n]\\) is the cost to get to the \\(n\\)th node</p> <pre><code>let d(u) be the cost to get to node u from the source/start\nlet d(v) be the cost to get to node v from the source/start\nlet c(u,v) be the cost to get from node u to v\n\nfor E-1 iterations\n    For each edge(u,v) in the graph\n        if d(u) + c(u,v) &lt; d(v)\n            then d(v) = d(u) + c(u,v)\n\n</code></pre> <p>The algorithm looks at each edge, determines whether taking that edge to the ending node will make the total distance to that node shorter.</p>"},{"location":"2-Constraint-Satisfaction-Problems/","title":"2 Constraint Satisfaction Problems","text":"<ul> <li>Defining the problem</li> <li>Scheduling courses example</li> <li>Backtracking Search<ul> <li>Algorithm         - Worst case</li> <li>Improving Backtracking search<ul> <li>#Improving Backtracking search#1. Use Least Remaining Variables (LRV) rule|1. Use Least Remaining Variables (LRV) rule</li> <li>#Improving Backtracking search#2. Use the largest number of active constraints|2. Use the largest number of active constraints</li> <li>#Improving Backtracking search#3. Choose the least constraining value for the variable|3. Choose the least constraining value for the variable</li> </ul> </li> </ul> </li> <li>Example:<ul> <li>Need to formulate a connected graph</li> </ul> </li> <li> <p>[Einstein's Puzzle]]</p> </li> <li> <p>Need to listen to I can't get no satisfcation... Constraint satisfaction deals with scheduling, such as timetable problems, operations, etc. Represented as a graph, where edges represent contraints. There is a domain for variables as well.</p> </li> </ul>"},{"location":"2-Constraint-Satisfaction-Problems/#defining-the-problem","title":"Defining the problem","text":"<p>You have variables which could be discrete or categorical (makes up the domain, which should be finite) Can use linear programming for random variable CSP Edges represent nodes that have constraints</p> <ul> <li>Unary: (self edge) is weird?</li> <li>Binary: paris of variables</li> <li>Can have ternary and more</li> </ul>"},{"location":"2-Constraint-Satisfaction-Problems/#scheduling-courses-example","title":"Scheduling courses example","text":"<p>Constraints:</p> <ul> <li>Professor schedules</li> <li>Courses on the same level</li> <li>Courses which students are likely to take in the same term</li> <li>Lecture hall availability</li> </ul> <p>Sudoku is a Constraint Satisfaction problem</p>"},{"location":"2-Constraint-Satisfaction-Problems/#backtracking-search","title":"Backtracking Search","text":"<p>A form of DFS </p> <p>Will use a country example. Domain: <code>{r,g,b,y}</code></p>"},{"location":"2-Constraint-Satisfaction-Problems/#algorithm","title":"Algorithm","text":"<ul> <li>Start with empty assignment of all variables (countries), or initial config</li> <li>Choose a random variable (country) and assign a value from the domain that doesn't break constraints<ul> <li>Ex. Assign the colour blue to Portugal</li> </ul> </li> <li>Repeat with different variable<ul> <li>Ex. Assign the colour red to Spain</li> </ul> </li> <li>If a variable is reached where there is no solution, then you must backtrack.<ul> <li>Go back to the previous variable, and reassign a new value from domain and try again</li> </ul> </li> </ul>"},{"location":"2-Constraint-Satisfaction-Problems/#worst-case","title":"Worst case","text":"<p>Will try every possible combination and it could take very long Can be \\(O(d^n)\\) time complexity, with \\(n\\) variables, and a domain of size \\(d\\).</p>"},{"location":"2-Constraint-Satisfaction-Problems/#improving-backtracking-search","title":"Improving Backtracking search","text":""},{"location":"2-Constraint-Satisfaction-Problems/#1-use-least-remaining-variables-lrv-rule","title":"1. Use Least Remaining Variables (LRV) rule","text":"<p>Choose the next variable with the fewest possible values left - This ensures that we fulfill as many variables as possible     - If we don't, then our tree could be very wide and that would result in a very high number of nodes</p>"},{"location":"2-Constraint-Satisfaction-Problems/#2-use-the-largest-number-of-active-constraints","title":"2. Use the largest number of active constraints","text":"<p>Only use this rule if LRV doesn't reduce the search time, or all the variables have the same number of constraints? (#2 is a tie-breaker rule)</p> <p>Choose the variable participating in the largest number of active constraints. This is equivalent to the node with the most connections to other nodes (what variable it chooses affects the greatest number of other variables). This reduces the domain for all the nodes connected to the given node.</p> <p>An active constraint is between variables that have yet to be assigned.</p>"},{"location":"2-Constraint-Satisfaction-Problems/#3-choose-the-least-constraining-value-for-the-variable","title":"3. Choose the least constraining value for the variable","text":"<p>How to choose a value for the variable we've chosen to process:</p> <ul> <li>If we have two nodes/variables which both share an edge:<ul> <li>S can be assigned \\({R,G,B}\\) and F can be assigned \\({R,G}\\)</li> <li>We choose \\(B\\)  for S because its the value that is the least likely to constrain F's domain.</li> </ul> </li> </ul>"},{"location":"2-Constraint-Satisfaction-Problems/#example","title":"Example:","text":"<ul> <li>5 houses in a row are a street<ul> <li>In each house lives a person with the following domains:</li> </ul> </li> <li>Nationalities: \\({China, British, Thai, Canadian, Indian}\\)</li> <li>Each person has a different favorite drink: \\({Tea, Coffee, Monster, Tequila, Beer}\\)</li> <li>Each person has a different pet: \\({Snake, Tarantula, Capybara, Cat, Fish}\\)</li> <li>Each person watches a different show: \\({Wednesday, The\\ Last\\ of\\ Us, Blue\\ Lock, Jack\\ Ryan, Arcane}\\)</li> <li>Each house has a different colour: \\({R, G, B, W, Y}\\)</li> <li>The ORDER of the houses matter (they must be in a row): \\({1,2,3,4,5}\\)</li> </ul> <p>Cannot use rule 1, as each user has the same amount of variables to choose from Cannot use rule 2, as everybody constrains everybody at the start Cannot use rule 3, as each person has the same domains</p>"},{"location":"2-Constraint-Satisfaction-Problems/#need-to-formulate-a-connected-graph","title":"Need to formulate a connected graph","text":"<ul> <li>We can draw a fully connected graph of the 5 houses</li> </ul>"},{"location":"2-Constraint-Satisfaction-Problems/#einsteins-puzzle","title":"Einstein's Puzzle","text":"<p>Let us assume that there are five houses of different colors next to each other on the same road. In each house lives a man of a different nationality. Every man has his favorite drink, his favorite brand of cigarettes, and keeps pets of a particular kind.</p> <ol> <li>The Englishman lives in the red house.</li> <li>The Swede keeps dogs.</li> <li>The Dane drinks tea.</li> <li>The green house is just to the left of the white one.</li> <li>The owner of the green house drinks coffee.</li> <li>The Pall Mall smoker keeps birds.</li> <li>The owner of the yellow house smokes Dunhills.</li> <li>The man in the center house drinks milk.</li> <li>The Norwegian lives in the first house.</li> <li>The Blend smoker has a neighbor who keeps cats.</li> <li>The man who smokes Blue Masters drinks bier.</li> <li>The man who keeps horses lives next to the Dunhill smoker.</li> <li>The German smokes Prince.</li> <li>The Norwegian lives next to the blue house.</li> <li>The Blend smoker has a neighbor who drinks water.</li> </ol> <p>The question to be answered is: Who keeps fish?</p>"},{"location":"2-Constraint-Satisfaction-Problems/#how-to-solve-a-tree-structured-csp-ts-csp","title":"How to solve a tree structured CSP (TS CSP)","text":"<p>Using topological sort in \\(O(Nd^{2})\\)</p> <ul> <li>Pick a root (normally the one with the most edges)</li> <li>List all nodes with an edge to the root<ul> <li>where each node is connected to 1 parent and same # of children</li> </ul> </li> <li>List domains for variables</li> <li>Backward pass (right to left or children to parent) example below<ul> <li>For each node, remove from parent any inconsistent assignment</li> </ul> </li> <li>Backtracking search (but will not backtrack because it will always have a valid assignment)</li> </ul>"},{"location":"2-Constraint-Satisfaction-Problems/#example-of-topological-sort","title":"Example of topological sort","text":"<p>![[Obsidian-Attachments/Pasted image 20230130092114.png]] ![[Obsidian-Attachments/Pasted image 20230130092200.png]]</p>"},{"location":"2-Constraint-Satisfaction-Problems/#backward-pass","title":"Backward pass","text":"<ul> <li>At H, remove \\(r\\) from G</li> <li>At G, has \\(g,b\\) so because if D chooses one of them, G can still choose the other. Therefore nothing needs to be removed from D</li> <li>At F, remove \\(b\\) from D</li> <li>At E, nothing is removed from D as in G</li> <li>At B, nothing is removed from A</li> <li>At A, nothing is removed from C</li> <li>At C, remove \\(y\\) from D</li> </ul>"},{"location":"2-Constraint-Satisfaction-Problems/#near-ts-csp","title":"Near TS CSP","text":"<p>![Obsidian-Attachments/IMG_8638 copy.png</p> <ul> <li>Need to convert it into a tree structured CSP<ul> <li>Can do this by removing cycles, starting with the nodes that belong to the most cycles</li> </ul> </li> <li>Pick K variables to instantiate (the number of nodes that need to be removed to eliminate all cycles)<ul> <li>Results in (n-k) variables in TS CSP</li> </ul> </li> <li>\\(O(d^{k}(n-k)d^{2})\\)<ul> <li>where \\(d^{k}\\) is how long it takes to satisfy the domains of \\(k\\) variables</li> </ul> </li> </ul>"},{"location":"2-Constraint-Satisfaction-Problems/#if-all-else-fails-perform-local-search","title":"If all else fails, perform Local Search","text":"<p>An iterative method that is okay, not optimal (sometimes not even close) but it is the best we can do.</p> <ul> <li>Can solve scheduling with it</li> </ul>"},{"location":"2-Constraint-Satisfaction-Problems/#basicsimple-method","title":"Basic/Simple Method","text":"<ul> <li>Set up problem as a set of variables to be assigned</li> <li>Pick an initial assignment at random for all the variables<ul> <li>Will have a lot of broken constraints</li> <li>Could be improved a lot</li> </ul> </li> <li>Do the following again and again until (\\(k\\) iterations with no improvement)<ul> <li>Pick a variable at random (could be improved)</li> <li>Change it's value at random</li> <li>Check number of broken constraints (could also create a cost fnc, where cost is higher for solving tougher constraints such as with smaller domains)<ul> <li>Keep if smaller</li> <li>Reject otherwise</li> </ul> </li> </ul> </li> </ul>"},{"location":"2-Constraint-Satisfaction-Problems/#improvement-beam-search","title":"Improvement: Beam Search","text":"<p>If we have a schedule for a previous version of the constraint tree, we only have to optimize for the changes to it for the new version </p> <p>There are local minima everywhere     Keep multiple guesses and optimize     Choose the best at the end</p> <p>Or run K times and pick the best configuration</p>"},{"location":"2-Constraint-Satisfaction-Problems/#n-queens-problem","title":"N Queens problem","text":"<ul> <li>Can be done with backtracking search, but this is not optimal</li> </ul> <p>Can be set up smartly as follows:</p> <ul> <li>Ensure no two queens are on the same row or column</li> <li>Keep track of how many conflicts each queen has<ul> <li>Choose the one with the most conflicts and reassign that one first</li> <li>Can move that queen to the row with the fewest conflicts</li> </ul> </li> <li>This supposedly converges very quickly</li> </ul>"},{"location":"2-Constraint-Satisfaction-Problems/#can-local-search-work-on-sudoku","title":"Can local search work on sudoku?","text":"<p>Probably not, local search will not produce an optimal solution Sudoku has many optimal solutions (he said?)</p>"},{"location":"2-Constraint-Satisfaction-Problems/#deterministic-annealing","title":"Deterministic annealing","text":"<p>Keep track of a temperature \\(T\\) which is initially large</p> <p>Run local search</p> <ul> <li>Initialize at random</li> <li>Loop<ul> <li>If soln is better, then keep</li> <li>Else, keep with probability proportional to temperature</li> <li>Decrease temperature</li> </ul> </li> </ul> <p>Way worse than a normal local search problem, i dont really understand this problem though</p>"},{"location":"2-Constraint-Satisfaction-Problems/#local-search-for-k-medians","title":"Local search for K-medians","text":"<ol> <li>Pick k random points to be the centroid</li> <li>Explore the neighbourhood by moving those centroids<ol> <li>Don't need to be too clever, </li> </ol> </li> </ol>"},{"location":"3-Game-Playing/","title":"3 Game Playing","text":"<p>We will take about 2 player adversarial games</p> <p>Like a search, you need to define configurations/states of being so the agent knows how to move and reach some goal state.</p>"},{"location":"3-Game-Playing/#example-tic-tac-toe","title":"Example Tic Tac Toe","text":"<ul> <li>Goal state: three of a X/O in a row</li> <li>Actions<ul> <li>9 possible edges to travel to initially, as you can play in any of the actions</li> <li>8 moves subsequently</li> </ul> </li> <li>This will however assume that the opponent will move exactly as we predict</li> </ul> <p>To do this properly, we will probably need to define some heuristic. But it would be almost impossible to win because we'd have to rerun the search if the opponent plays something unexpected.</p>"},{"location":"3-Game-Playing/#possible-strategy","title":"Possible strategy","text":"<p>Run djikstra's and pick the option that gives you the most paths to the goal state.</p>"},{"location":"3-Game-Playing/#minimax-search","title":"MiniMax Search","text":"<p>Uses BFS!</p> <p>Utility Function: It will score all configurations, where it will assign +1 if it results in a P1 win, and -1 if it results in a loss for P1, 0 if draw.</p> <p>P1 wants to maximize the utility function P2 wants to minimize the utility function </p>"},{"location":"3-Game-Playing/#for-tic-tac-toe","title":"For Tic Tac Toe","text":"<p>P1 gets the first move, and will try to maximize the utility function  P2 on the second move, will try to minimize that same utility function At a goal state, depending on which player has the move to get to the goal state, we can backtrack and fill out all the utility values for each configurations.     If the goal state is a loss, and P2 has the move that can get there, then that node at P2 has a utility of -1.</p>"},{"location":"3-Game-Playing/#complexity","title":"Complexity","text":"<p>\\(O(b^{m})\\) Where \\(m =\\) number of moves \\(b=\\) branching factor</p> <p>Ex. in chess, the branching factor is 25 and there are approx 100 moves</p>"},{"location":"3-Game-Playing/#method","title":"Method","text":"<p>Starting at the current configuration, perform minimax for at most k levels of the search tree (where k depends on how much time we have to move)</p> <p>At level K, we use the utility function to score the node (will be a score for an incomplete game, so the utility function will be crunchy)</p> <p>To make this more efficient, we can perform tree pruning, specifically:</p>"},{"location":"3-Game-Playing/#alpha-beta-pruning","title":"\\(\\alpha, \\beta\\) pruning","text":"<p>Each node in the tree is going to be associated with \\(\\alpha,\\beta\\) values that are large numbers. (Ex \\(\\alpha=-\\infty, \\beta=+\\infty\\)) The current node will inherit these values from it's parent, and need to perform minimax on it's children nodes in order to update these values.</p> <p>\\(\\alpha, \\beta\\) keeps track of the best current choice for current node and the parents node</p> <ul> <li>The max nodes update their \\(\\alpha\\) value and prune based on the \\(\\beta\\) value.</li> <li>The min nodes update \\(\\beta\\) value and prune based on \\(\\alpha\\).<ul> <li>Eg. The \\(\\beta\\) value tracks the best current choice, and will report it back up the tree. On the parents node, it will set \\(\\alpha\\) to that value.</li> </ul> </li> </ul>"},{"location":"3-Game-Playing/#example","title":"Example","text":"<p>When you go to another child (min) node, you will pass in the \\(\\alpha, \\beta\\) values from the parent (max) and if that child(min) sets their \\(\\beta\\) at some value that is smaller than \\(\\alpha\\), then it can immediately stop (as the min node will not give any value \\(\\beta\\) greater than that minimum)</p> <p></p>"},{"location":"3-Game-Playing/#monte-carlo-tree-search","title":"Monte Carlo Tree Search","text":"<p>Not really talked about, just a brief description of it and how it isn't great</p> <p>Runs simulations at each move and uses the result to create some utility function</p>"},{"location":"3-Game-Playing/#expectiminimax-games-with-chance","title":"ExpectiMiniMax - Games with Chance","text":"<p>Where there is a random chance event after each move (picking a card, rolling a die) and that influences the next state/final states.</p> <p>In the tree search, it is represented with a diamond shaped node called a chance node which leads to child nodes with certain probabilities.</p> <p>What we do is calculate the expected utility at each move for each player, using the simple weighted average technique. </p> <p>We can still do \\(\\alpha, \\beta\\) pruning! Heres how:</p>"},{"location":"3-Game-Playing/#alpha-beta-pruning_1","title":"Alpha Beta Pruning","text":"<p>You cannot compute the average utility if you don't compute the average utility of every child node of the chance nodes, because you cannot take the average with missing values.</p> <p>In practice the utility function is always bounded. </p> <p>The overhead of computing with \\(\\alpha, \\beta\\) is just proportional to \\(N\\), which is still less compute than \\(b^{k}\\) or \\(b^{m}\\). </p>"},{"location":"4-Reinforcement-Learning/","title":"4 Reinforcement Learning","text":"<p>We want to get around having a complicated utility function. To do this, we can train our agent to learn a utility function. Afterward, we can then get the agent to play the game/task they were trained to do.</p>"},{"location":"4-Reinforcement-Learning/#the-setup","title":"The setup","text":"<p>A problem characterized by a set of state variables, which constitute configurations \\(s\\) contained in the entire state space \\(S\\). This state space contains end states, where the agent has solved the problem and states where the agent failed.</p> <p>We also need a set of all actions \\(A\\) that agents can take.</p> <p>Finally, we need a reward function/reinforcement signal that maps states \\(s\\) to a reward/punishment. (The goal is to no have to engineer this reward function)</p> <p>This assumes discrete time steps in this problem.</p>"},{"location":"4-Reinforcement-Learning/#markov-decision-process-mdp","title":"Markov Decision Process (MDP)","text":"<p>We care about the immediate past and only that. We don't care about the longterm past.</p> <p>Setup: The agent will learn a policy \\(\\Pi\\) that maps states \\(s\\) to an optimal action \\(a\\). We also assume the environment is stationary. </p> <p>Why do we need to assume the environment is stationary? That is, the environment does not change. We need this because otherwise there will be a very very large state space that the agent will have to learn over, becoming infeasible.</p> <p>Thats not to say that we cannot have an environment that is non-stationary, it just means that it will be more difficult for the agent to learn.</p>"},{"location":"4-Reinforcement-Learning/#the-learning-policy","title":"The Learning Policy","text":"<p>Goal: Maximize the expected sum $$ \\mathbb{E}\\left[ \\sum{\\infty}_{t=0}\\gamma{t}r_{t} \\right] \\quad \\gamma \\in [0,1) $$</p> <p>Where \\(r_{t}\\) is the reward at a given time step, and \\(\\gamma_{t}\\) is a discount factor.</p> <p>We don't like that infinite sum, so we can rewrite it.</p> <p>Define a quantity \\(V^{*}(s) =\\) value of state s \\(= \\max_{\\Pi}\\mathbb{E}\\left[ \\sum^{\\infty}_{t=0}\\gamma^{t}r_{t} \\right]\\)</p> <p>This is the max expected value of a given state \\(s\\), indicating whether the state is good or not informing the quality.</p> <p>It can be computed under the process:</p> <ul> <li>Given a state \\(s\\) the agent will take an action \\(a\\), receive a reward \\(r\\), and end up at some state \\(s'\\) . This can be used to compute \\(V^{*}(s)\\)<ul> <li>\\(&lt;s,a,r,s'&gt;\\)</li> </ul> </li> </ul> <p>Rewrite </p> \\[ \\begin{aligned} V^{*}(s) &amp;= max_{a}\\left( R(s,a) +\\\\ \\gamma \\sum_{s'\\in S \\text{ reachable from }s}T(s,a,s')V^{*}(s')\\right)\\\\ \\text{Where }&amp; R(s,a) \\text{ is the Reward from taking action a from state s}\\\\ &amp;T(s,a,s') \\text{is the Transition fnc: prob of ending up at s' from s under action }a\\\\ &amp;V^{*}(s') \\text{ is the value at the neighbour of } s \\end{aligned} \\] <p>Which gets rid of the infinite sum.</p>"},{"location":"4-Reinforcement-Learning/#value-iteration","title":"Value Iteration","text":"<p>Initialize \\(V^{*}(s) = 0\\), and a table \\(Q(s,a)=\\emptyset\\) </p> <p>Repeat until \\(\\Pi\\) is good enough.</p> <pre><code>for all s in S\n    for all a in A\n        Q(s,a) = R(s,a) + gamma \\sum_{s' \\in S} T(s,a,s')V*(s')\n\n    update V*(s) = max_a Q(s,a)\n    update Pi(s) = argmax_a Q(s,a)\n</code></pre> <p>At 1st iteration, \\(V(s) \\ne \\emptyset\\) for configs where we get some immediate \\(R(s,a)\\).</p> <p>How to stop: We need to check for convergence of \\(V^{*}(s)\\), but it could still have very small changes at every iteration. Instead, we can check for when the policy \\(\\Pi\\) has stopped changing.</p> <p>This training can run offline (not during game time) so there are no time constraints.</p> <p>After training, we get a look up table for actions to take, so given a state \\(s\\) we select a few \\(a\\) from \\(\\Pi(s)\\).</p>"},{"location":"4-Reinforcement-Learning/#estimating-the-transition-function","title":"Estimating the Transition function","text":"<p>If we want the real transition function:</p> <ul> <li>We could to run the real thing, but thats time consuming, slow, and could be affected by the environment.</li> <li>We could run a simulation, but that could probably not generalize well.</li> </ul> <p>We can actually just tweak the learning process (value iteration), to get it.</p>"},{"location":"4-Reinforcement-Learning/#q-learning","title":"Q-learning","text":"<p>Goal: Try to ignore the transition function We still have to design the reward function ourselves</p> <p>Initially, set:</p> <ul> <li>\\(Q(s,a)=0\\)</li> <li>\\(\\Pi(s) =\\) random action</li> <li>Determine initial state \\(s\\)</li> <li>Set \\(\\gamma\\) to whatever, and new param \\(\\alpha\\) which represents a gradient to smth that should be small</li> </ul> <pre><code>N = a lot of training rounds\nfor (i in 1...N):\n    from current state, get random action\n    simulate agent doing the action, and sees what happens\n        - find out the reward r(s,a)\n        - find out next state s', which becomes cur state\n            - if end game, choose a random state s as cur state\n\n    # preform gradient descent (the second and third terms)\n    Q(s,a) += alpha * [r(s,a) + \n                       gamma * max_{a'} Q(s',a') - \n                       Q(s,a)]\nupdate policy Pi(s) for all s\n</code></pre> <p>All of this requires a lot of careful sampling (not sampling same nodes, not getting stuck in a strongly connected subset of the graph)</p>"},{"location":"4-Reinforcement-Learning/#improvement-to-q-learning","title":"Improvement to Q-learning","text":"<p>We have \\(K\\) rounds of training:</p> <pre><code>for (j in 0...K):\n    do prev code\n\n    in round j of K:\n        choose randomly action a with P(a) = 1-j/K\n    else:\n        choose action in Pi(s)\n</code></pre> <p>This basically chooses a random action a less and less of the time as the number of rounds goes on, as this implies that as we continue the learning, the policy should be relied on more as it should contain the best action.</p> <p>This whole q-learning thing can be hard because:</p> <ul> <li>the state space can be huge.</li> <li>We need to build an actual simulator</li> <li>the agent will be trained on a specific state space/environment, but given a new one it may not do well (not generalized)</li> </ul>"},{"location":"4-Reinforcement-Learning/#feature-based-q-learning","title":"Feature-based Q-Learning","text":"<p>We can generalize using feature based Q-learning</p> <p>We create a set of abstracted features that can be used to estimate the reward from a certain state \\(Q(s)\\). This is different form before as its not a table, it is:</p> \\[ Q(s) = \\sum_{i}w_{i}f_{i}(s) \\] <p>A weighted sum of the features we can evaluate for a given state. These features will work on any maze, and aren't tied to any specific seed. The weights will need updating.</p> <p>Instead of updating \\(Q(s,a)\\) in the Q-learning loop, we have</p> <pre><code>w_t += alpha * f_t (s) * [r + gamma Q(s') - Q(s)]\n</code></pre> <p>Which just updates the weights, but doesn't actually update the policy. yet.</p>"},{"location":"5-Neural-Nets/","title":"5 Neural Nets","text":"<p>Fairly straightforward; we'll learn how to train a basic neural net from scratch and such.</p>"},{"location":"5-Neural-Nets/#mcculloch-pitts-model","title":"McCulloch-Pitts model","text":"<p>You have a set of inputs \\(I_{1}, I_{2}, \\dots I_{k}\\) which are then each multiplied by respective scalars \\(w_{1},w_{2},\\dots w_{k}\\) and they are summed up. If that sum is greater than some threshold value, then a 1 is outputted.</p>"},{"location":"5-Neural-Nets/#perceptron","title":"Perceptron","text":"<p>This takes a McCulloch-Pitts model as a single perceptron, that preforms the sum and threshold of inputs and returns some output. It is a single unit of processing in a traditional neural net.</p> <p>Outputting some value given some threshold is called the activation function. This is the difference between a McCulloch-Pitts model, as it can be more complicated than simply being greater than some threshold.</p>"},{"location":"5-Neural-Nets/#example","title":"Example","text":"<p>Suppose we want to detect a pattern in an image. The image is 4x4.</p> Col 1 Col 2 Col 3 Col 4 X X X X X <p>How should we set the weights and thresholds properly? </p> <p>Threshold: 5 Weights:</p> <ul> <li>1 if X</li> <li>-1 for all other cells (so if they are activated/have X, then the sum would subtract 1)</li> </ul> <p>This is so we can exactly identify the specific pattern, and it will only activate when the correct pixels are X.</p> <p>But this is still too strict, to make it more accepting of general answers, we need to change the activation function to return smoother values than just 0 or 1.</p>"},{"location":"5-Neural-Nets/#activation-functions","title":"Activation functions","text":""},{"location":"5-Neural-Nets/#the-logistic-function","title":"The logistic function","text":"\\[ f(x) = \\frac{1}{1+e^{-\\alpha x}} \\] <pre><code>top=1.5;bottom = -0.5;\n---\ny=1/(1+e^{-x})\n</code></pre>"},{"location":"5-Neural-Nets/#hyperbolic-tangent","title":"Hyperbolic Tangent","text":"<p>Useful for linear regression as it outputs negative values</p> <pre><code>top=1.5;bottom = -1.5;\n---\ny=(e^{x}-e^{-x})/(e^{x}+e^{-x})\n</code></pre> <p></p>"},{"location":"5-Neural-Nets/#rectified-linear-unit-relu","title":"Rectified Linear Unit (ReLU)","text":"<p>Linear after some threshold</p> <pre><code>y=0| x&lt;0\ny=x| x&gt;0\n</code></pre> <p></p>"},{"location":"5-Neural-Nets/#addressing-thresholds","title":"Addressing thresholds","text":"<p>We can add a bias input that can shift the function without actually having to change the activation function.</p>"},{"location":"5-Neural-Nets/#training","title":"Training","text":"<p>The process is a form of reinforcement learning! </p> <ul> <li>We show the input to the network</li> <li>Compute the output which would give us some error (compared to the true result). </li> <li>We then iterate</li> <li>Correct and modify the weights to continuously reduced the error (could be squared error)</li> </ul>"},{"location":"5-Neural-Nets/#how-to-adjust-weights","title":"How to adjust weights?","text":"<p>Need to find  $$ \\frac{ \\partial err }{ \\partial w_{ab} }  $$</p> <p>Where \\(a\\) is the index of an input, \\(b\\) is an index of a neuron connected to the input.</p> <p>We can therefore decompose it to:  $$ \\frac{ \\partial A(I) }{ \\underbrace{ \\partial w_{ab} }{ I{a} } } \\underbrace{ \\frac{ \\partial Output_{A} }{ \\partial A(I) } }{ \\text{activation fnc deriv} } \\frac{ \\partial Err{A} }{ \\partial Output_{A} }  $$</p> <p>where $$ A(I) = \\sum_{i}w_{i}I_{i} $$</p> <p>The third term may give problems if its squared error, so often a different type of error metric is used.</p>"},{"location":"5-Neural-Nets/#error-backpropogation","title":"Error backpropogation","text":"<p>Which we can use to create a gradient descent function, where \\(\\alpha\\) is some small learning rate.</p> \\[ w_{ab} = w_{ab} + \\alpha \\frac{ \\partial Err_{b} }{ \\partial w_{ab} }  \\]"},{"location":"5-Neural-Nets/#stochastic-gradient-descent-sgd","title":"Stochastic Gradient Descent (SGD)","text":"<p>To make weight updates more efficient, you can take a batch of inputs of size \\(k\\), accumulate the squared error, and then does one big weight update for the \\(k\\) weights.</p>"},{"location":"5-Neural-Nets/#issues-with-the-activation-function","title":"Issues with the activation function","text":"<ul> <li>Saturation: For example, with the logistic function, beyond 5 and -5 (or whatever limits) will simply give 0 gradient.<ul> <li>Causes the learning to halt</li> <li>Dealt with by resetting the weights and trying again and working between the two limits</li> <li>Reset when the weights of the last few iterations show no changes</li> </ul> </li> </ul>"},{"location":"5-Neural-Nets/#multi-layer-networks","title":"Multi-layer networks","text":"<p>Fully connected multi-layer networks are only good for small problems, for larger problems it quickly becomes unwieldy and too complicated to learn.</p> <p></p> <p>The inner layers of the network are meant to learn \"features\" of the data.</p> <p>The learning/updating of the weights are now also more complicated.</p> \\[ \\frac{\\partial E r r_b}{\\partial O_b}=\\sum_{j \\in \\text { neurons connected to } B} w_{b j} \\frac{\\partial f(x)}{\\partial A_j(I)} \\cdot\\left(T_j-O_j\\right) \\] <p>If we find any weights that grow too fast, or don't change much at all, we can cut those weights out (exploding gradient problem).</p>"},{"location":"6-Bayes-Nets/","title":"6 Bayes Nets","text":"<p>last week</p>"}]}